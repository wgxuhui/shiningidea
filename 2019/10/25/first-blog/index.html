<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>低光照图像增强文献整理2019.10 | Her Technical Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Low-Light Image Enhancement via a Deep Hybrid Network  EnlightenGAN：Deep Light Enhancement without Paired Supervision  Seeing Motion in the Dark  1. Low-Light Image Enhancement via a Deep Hybrid N">
<meta name="keywords" content="文献整理,微光增强">
<meta property="og:type" content="article">
<meta property="og:title" content="低光照图像增强文献整理2019.10">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2019&#x2F;10&#x2F;25&#x2F;first-blog&#x2F;index.html">
<meta property="og:site_name" content="Her Technical Blog">
<meta property="og:description" content="Low-Light Image Enhancement via a Deep Hybrid Network  EnlightenGAN：Deep Light Enhancement without Paired Supervision  Seeing Motion in the Dark  1. Low-Light Image Enhancement via a Deep Hybrid N">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;github.com&#x2F;wgxuhui&#x2F;wgxuhui.github.io&#x2F;blob&#x2F;master&#x2F;images&#x2F;Deep%20Hybrid%20Network.jpg?raw=true">
<meta property="og:image" content="https:&#x2F;&#x2F;github.com&#x2F;wgxuhui&#x2F;wgxuhui.github.io&#x2F;blob&#x2F;master&#x2F;images&#x2F;edge%20stream.png?raw=true">
<meta property="og:image" content="https:&#x2F;&#x2F;github.com&#x2F;wgxuhui&#x2F;wgxuhui.github.io&#x2F;blob&#x2F;master&#x2F;images&#x2F;enlightenGan.JPG?raw=true">
<meta property="og:image" content="https:&#x2F;&#x2F;github.com&#x2F;wgxuhui&#x2F;wgxuhui.github.io&#x2F;blob&#x2F;master&#x2F;images&#x2F;seeing%20motion%20in%20the%20dark.jpg?raw=true">
<meta property="og:updated_time" content="2019-11-05T04:07:33.047Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;github.com&#x2F;wgxuhui&#x2F;wgxuhui.github.io&#x2F;blob&#x2F;master&#x2F;images&#x2F;Deep%20Hybrid%20Network.jpg?raw=true">
  
    <link rel="alternate" href="/atom.xml" title="Her Technical Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Her Technical Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">xuhui_ustc@163.com</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-first-blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/10/25/first-blog/" class="article-date">
  <time datetime="2019-10-25T08:54:12.000Z" itemprop="datePublished">2019-10-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      低光照图像增强文献整理2019.10
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<blockquote>
</blockquote>
<ul>
<li>Low-Light Image Enhancement via a Deep Hybrid Network </li>
<li>EnlightenGAN：Deep Light Enhancement without Paired Supervision </li>
<li>Seeing Motion in the Dark</li>
</ul>
<h3 id="1-Low-Light-Image-Enhancement-via-a-Deep-Hybrid-Network"><a href="#1-Low-Light-Image-Enhancement-via-a-Deep-Hybrid-Network" class="headerlink" title="1. Low-Light Image Enhancement via a Deep Hybrid Network"></a>1. Low-Light Image Enhancement via a Deep Hybrid Network</h3><p><em>Ren W , Liu S , Ma L , et al. Low-Light Image Enhancement via a Deep Hybrid Network[J]. IEEE Transactions on Image Processing, 2019:1-1.</em></p>
<p>该文章用一种可训练的混合网络来增强在昏暗环境中的图像质量。由于之前采用的方法通常是对于图像的整体特征进行学习，这种方法会漏掉一些细节。因此这篇文章采用了混合网络架构，由两个不同的数据流组成，一个数据流通过编码器-解码器网络学习图像的全局内容，另一个数据流则用于学习图像边缘细节信息。这种方法类似于传统的反锐化掩模方法，即分离图像边缘细节，增强后再与原图像进行融合。<br><img src="https://github.com/wgxuhui/wgxuhui.github.io/blob/master/images/Deep%20Hybrid%20Network.jpg?raw=true" alt="网络结构"><br>内容流采用了残差网络，并在网络的前两层采用了空洞卷积（dilated convolution）来扩展接收域。在网上查了下空洞卷积的好处，就是可以在“不做pooling损失信息的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息。”当然也有坏处，比如会损失信息的连续性，但是因为这个网络里还有一个边缘流，所以可能这方面就不太重要了吧。另外在上采样阶段，这个残差网络还加入了边缘流的信息，以获得更多的细节。<br>边缘流采用的是空间变异RNN（spatially variant RNN），这种方法是2016年在ECCV首次提出，RNN可以模拟一个图像的滤波操作，用DeepCNN训练一个权值p，将隐藏状态转换成当前状态（h[k-1] -&gt; h[k])。但该滤波器是一个归一化的滤波器，也就是增益为1，因此学习出的边缘仍然很黑。所以他们提出了一种改进空间变异RNN，用DeepCNN学习g,p两个参数。h[k]=g[k]x[k]+p[k]h[k-1]。他们在四个方向上都进行了这种计算。<br><img src="https://github.com/wgxuhui/wgxuhui.github.io/blob/master/images/edge%20stream.png?raw=true" alt="边缘流的网络"><br>损失函数：MSE损失（ MSE loss）、对抗损失（adversarial loss）和感知损失（perceptual loss）。感知损失采用VGG-16，对抗损失采用discriminator。最后再把三个损失函数加起来。<br>数据集：MIT-Adobe FiveK。从5000张图片找到336张低光照图片对，这种低光照指的是在日出或夕阳时的照片。图片加了γ=3的伽马校正和1%的高斯噪声。对比了WVM、LIME、HDRNet、DSLR-Q的方法。看结果，这种方法确实可以在增强图像质量的同时达到比较自然的效果，在PSNR/SSIM方面也有了一定提升。<br>感言时间：总体而言，这篇论文教会我的东西就是让我了解了太多网络，作为一个新手，感觉全世界的方法这文章都用上了，写的肯定很不容易，我看得也十分不易。此外自己加噪声造数据集的方法我觉得十分不可取，让人感觉像做了一个很复杂的模型，但一旦拉出来溜溜搞不好就要露馅。</p>
<h3 id="2-EnlightenGAN：-Deep-Light-Enhancement-without-Paired-Supervision"><a href="#2-EnlightenGAN：-Deep-Light-Enhancement-without-Paired-Supervision" class="headerlink" title="2. EnlightenGAN： Deep Light Enhancement without Paired Supervision"></a>2. EnlightenGAN： Deep Light Enhancement without Paired Supervision</h3><p><em>Jiang, Y., Gong, X., Liu, D., Cheng, Y., Fang, C., Shen, X., … &amp; Wang, Z. (2019). EnlightenGAN: Deep Light Enhancement without Paired Supervision. arXiv preprint arXiv:1906.06972.</em></p>
<p>本文采用无监督学习的对抗生成网络进行图像增强的处理。这篇文章首先提出了以往基于深度学习的图像增强方法的弊端，包括：1)同一视觉场景的败坏图像与地面真值图像(如低光图像对与常光图像对同时捕获)是非常困难的，甚至是不切实际的;2)很多文章从普通图像中合成低光照图像，但这种合成的结果通常不够逼真，导致训练后的模型应用于真实的微光图像时会产生各种伪影;3)在低光图像中可能没有唯一的或定义良好的高光地面真值。针对这些问题，作者采用了GAN网络在低光照图像和正常光图像空间建立非成对映射，而不需要依赖精确成对的图像，使得数据集不必使用合成数据或十分有限的数据量。<br><img src="https://github.com/wgxuhui/wgxuhui.github.io/blob/master/images/enlightenGan.JPG?raw=true" alt="网络结构"><br>首先网络使用了一个generator和两个discriminator作为对抗网络，两个discriminator分别检测整个图像和局部图像的true和false。对于生成器，采用的是U-Net网络，这是一种在很多低级的图像任务中都会用来做训练的网络。为了局部增强图像的亮度，用1-I作为生成器输出的mask，I为归一化后的图像像素，这样就相当于有选择性地增强了暗处。</p>
<p>文章的主要创新在于损失函数的设定，损失函数是用于鉴别器辨别真伪图像。与上一篇文章类似，这里同样用到了感知损失和对抗损失。但是研究人员在实验中发现感知损失随光照强度的变化不明显，也就是说感知损失是用于约束光强之外的特征，称为称其为自特征保留损失。原本的 VGG-feature distance 是网络输出和ground truth之间的距离，现在文中改成了输出和输入的距离，这样就可以在摆脱ground truth的情况下计算感知损失。另外文章还特别强调了在无groundtruth的情况下进行学习，需要注意自正则化的设定。最终的损失函数有四个，分别是全局对抗损失、局部对抗损失、全局自特征保留损失、局部自特征保留损失。</p>
<p>文章使用的数据集是Berkeley Deep Driving (BBD-100k)，在合成图像上看着这种方法确实很自然，但是在真实的夜景路拍情况下我并没有看出明显的优势。另外文章还把各种方法处理后的图像送入resnet-50分类器，本文方法比其他方法的分类正确率有微小的提升。</p>
<p>感言时间：这篇文章通过对损失函数的改进摆脱了对groundtruth的依赖，我觉得对于微光图像的增强还是很有意义的。</p>
<h3 id="3-Seeing-Motion-in-the-Dark"><a href="#3-Seeing-Motion-in-the-Dark" class="headerlink" title="3. Seeing Motion in the Dark"></a>3. Seeing Motion in the Dark</h3><p>该文章的目标是低光照下的视频处理。<br>这篇UIUC和Intel联合发表的论文和他们之前18年的工作（<em>Chen, C., Chen, Q., Xu, J., &amp; Koltun, V. (2018). Learning to see in the dark. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3291-3300).</em>）紧密相连。在18年的文章里，作者为了研究在低级光照下的图像质量增强，提出了一种端到端训练模式的全卷积网络FCN ，用于处理低亮度图像。引入了一个数据集，包含有原始的低曝光率、低亮度图片，同时还有对应的长曝光率图像。方法能够有效地抑制图像噪声，生成色彩均衡、逼真的图像。</p>
<p>在19年的这篇文章里，他们如法炮制，开始研究视频的图像增强。他们又采集了一个视频数据集，包括静态和动态两部分，静态数据集会采集对应的groundtruth长曝光数据用于训练，而动态视频作为测试集，且没有相应groundtruth，他们认为模型有能力从静态泛化到动态视频上。数据是原始raw视频，首先经过了比较复杂的预处理，然后送入神经网络。网络模型采用的是ResU-Net的结构，提出了重建损失和自一致性损失。其中重建损失是约束output与groundtruth之间的距离，自一致性损失是为了确保时序一致性。结论就是他们的模型确实可以在动态视频上达到比较好的效果。但是模型也并不是完美，比如会在午夜的月光视频中失效（虽然我不理解是为啥），另外就是他们的视频动态范围不高，其实算是降低了难度。<br><img src="https://github.com/wgxuhui/wgxuhui.github.io/blob/master/images/seeing%20motion%20in%20the%20dark.jpg?raw=true" alt="网络结构"><br>感言时间：这个工作让人感觉做深度学习真简单，只要拍点照片、视频就好了，做底层图像处理连人工打标签的工作都省了。不过我不能说这个工作没有意义，恰恰相反其实比那些复杂的网络有意义多了，至少给我们提供了很好用的数据集。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/10/25/first-blog/" data-id="ck35kg32s0001t0vocgqp4tyj" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BE%AE%E5%85%89%E5%A2%9E%E5%BC%BA/" rel="tag">微光增强</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%96%87%E7%8C%AE%E6%95%B4%E7%90%86/" rel="tag">文献整理</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/11/06/ICCV2019-1/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          ICCV2019文献整理1
        
      </div>
    </a>
  
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/SPAD/" rel="tag">SPAD</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%BE%E5%83%8F%E5%8E%BB%E5%99%AA/" rel="tag">图像去噪</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BE%AE%E5%85%89%E5%A2%9E%E5%BC%BA/" rel="tag">微光增强</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%87%E7%8C%AE%E6%95%B4%E7%90%86/" rel="tag">文献整理</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%81%A2%E5%A4%8D/" rel="tag">超分辨率恢复</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/SPAD/" style="font-size: 10px;">SPAD</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%8E%BB%E5%99%AA/" style="font-size: 10px;">图像去噪</a> <a href="/tags/%E5%BE%AE%E5%85%89%E5%A2%9E%E5%BC%BA/" style="font-size: 10px;">微光增强</a> <a href="/tags/%E6%96%87%E7%8C%AE%E6%95%B4%E7%90%86/" style="font-size: 20px;">文献整理</a> <a href="/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%81%A2%E5%A4%8D/" style="font-size: 10px;">超分辨率恢复</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/11/06/ICCV2019-1/">ICCV2019文献整理1</a>
          </li>
        
          <li>
            <a href="/2019/10/25/first-blog/">低光照图像增强文献整理2019.10</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Xuhui Wang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>