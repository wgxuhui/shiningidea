<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Her Technical Blog</title>
  
  <subtitle>xuhui_ustc@163.com</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-11-12T03:52:50.399Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Xuhui Wang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ICCV2019文献整理1</title>
    <link href="http://yoursite.com/2019/11/06/ICCV2019-1/"/>
    <id>http://yoursite.com/2019/11/06/ICCV2019-1/</id>
    <published>2019-11-06T01:05:12.000Z</published>
    <updated>2019-11-12T03:52:50.399Z</updated>
    
    <content type="html"><![CDATA[<hr><blockquote></blockquote><ul><li>Asynchronous Single-Photon 3D Imaging</li><li>Real Image Denoising with Feature Attention</li></ul><h3 id="1-Asynchronous-Single-Photon-3D-Imaging"><a href="#1-Asynchronous-Single-Photon-3D-Imaging" class="headerlink" title="1. Asynchronous Single-Photon 3D Imaging"></a>1. Asynchronous Single-Photon 3D Imaging</h3><p><em>Gupta A, Ingle A, Gupta M. Asynchronous Single-Photon 3D Imaging[C]//Proceedings of the IEEE International Conference on Computer Vision. 2019: 7909-7918.</em><br>这篇文章是Best Paper Honorable Mentions，但讲的东西么真的是很神奇。SPAD-单光子雪崩二极管，可以记录单个光子的到达时间，所以一般用飞行时间法去进行3D成像。虽然我没用过，但我上网搜了一下，貌似不是个全新的东西，国外已经有了商业化的产品。这篇文章讲的是怎样在有自然光的情况下使用SPAD，主要就是用了平移窗口进行收集，然后将峰值对齐之后，再把相当于本底噪声的自然光减掉。在外行看来这玩意好像并不高深，当然外行看得并不太懂，不知道是哪一点打动了评委得了最佳荣誉论文奖，有没有大神能科普下。<br><img src="https://github.com/wgxuhui/wgxuhui.github.io/blob/master/images/SPAD.JPG?raw=true" alt="Imaging model of single-photon 3D cameras"></p><h3 id="2-Real-Image-Denoising-with-Feature-Attention"><a href="#2-Real-Image-Denoising-with-Feature-Attention" class="headerlink" title="2. Real Image Denoising with Feature Attention"></a>2. Real Image Denoising with Feature Attention</h3><p><em>Anwar S, Barnes N. Real Image Denoising with Feature Attention[J]. arXiv preprint arXiv:1904.07396, 2019.</em><br>这篇文章主要是采用了很深的CNN和跳过链接组成的神经网络，解决常规的神经网络中合成数据去噪效果好但真实数据去燥效果不好的问题。主要贡献：1.他们的模型是第一个在去噪中加入特征注意的模型。2.模块化网络提升性能。3.合成数据集合真实数据集上都有较好的效果。<br>模型由特征提取、残差模块上的特征学习、残差重构三个主要模块组成。<br>网络结构：1. $$f_{0} = M_{e}(x)$$</p><p><a href="https://www.codecogs.com/eqnedit.php?latex=f_{0}&space;=&space;M_{e}(x)" target="_blank" rel="noopener"><img src="https://latex.codecogs.com/gif.latex?f_%7B0%7D&space;=&space;M_%7Be%7D(x)" alt="" title="f_{0} = M_{e}(x)"></a></p><p> 单层卷积提取初始特征得到$f_{0}，2. f_{r} = M_{fl}(f_{0})$学习残差模块EAM上的残差分量，网络深度很小，但提供了广泛的接收域，3. $\hat{y} = M_{r}(f_{r})$。损失函数就是简单地加权平均。采用m层EAM叠加特征抽取模块，再用另一个长跳过链接将输入图像送至输出模块以学习残差。<br><img src="https://github.com/wgxuhui/wgxuhui.github.io/blob/master/images/RIDNet.JPG?raw=true" alt="RIDNet"><br>特征注意模块：1. $g_{p}=\frac{1}{h\times w}\sum_{i=1}^{h}\sum_{i=1}^{w}f_{c}(i,j)$，全局平均池化，2.$r_{c} = \alpha (H_{U} (\delta (H_{D}(g_{p}))))$自门限机制，参考^[Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018. ]，具体没有看懂。<br>数据集：合成数据有BSD500，DIV2K，MIT-Adobe FiveK，真实数据集Nam，SSID。效果以PSNR及人眼为准。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;blockquote&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Asynchronous Single-Photon 3D Imaging&lt;/li&gt;
&lt;li&gt;Real Image Denoising with Feature Attention&lt;/li&gt;
&lt;/u
      
    
    </summary>
    
    
    
      <category term="文献整理" scheme="http://yoursite.com/tags/%E6%96%87%E7%8C%AE%E6%95%B4%E7%90%86/"/>
    
      <category term="SPAD" scheme="http://yoursite.com/tags/SPAD/"/>
    
      <category term="图像去噪" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E5%8E%BB%E5%99%AA/"/>
    
  </entry>
  
  <entry>
    <title>低光照图像增强文献整理2019.10</title>
    <link href="http://yoursite.com/2019/10/25/first-blog/"/>
    <id>http://yoursite.com/2019/10/25/first-blog/</id>
    <published>2019-10-25T08:54:12.000Z</published>
    <updated>2019-11-05T04:07:33.047Z</updated>
    
    <content type="html"><![CDATA[<hr><blockquote></blockquote><ul><li>Low-Light Image Enhancement via a Deep Hybrid Network </li><li>EnlightenGAN：Deep Light Enhancement without Paired Supervision </li><li>Seeing Motion in the Dark</li></ul><h3 id="1-Low-Light-Image-Enhancement-via-a-Deep-Hybrid-Network"><a href="#1-Low-Light-Image-Enhancement-via-a-Deep-Hybrid-Network" class="headerlink" title="1. Low-Light Image Enhancement via a Deep Hybrid Network"></a>1. Low-Light Image Enhancement via a Deep Hybrid Network</h3><p><em>Ren W , Liu S , Ma L , et al. Low-Light Image Enhancement via a Deep Hybrid Network[J]. IEEE Transactions on Image Processing, 2019:1-1.</em></p><p>该文章用一种可训练的混合网络来增强在昏暗环境中的图像质量。由于之前采用的方法通常是对于图像的整体特征进行学习，这种方法会漏掉一些细节。因此这篇文章采用了混合网络架构，由两个不同的数据流组成，一个数据流通过编码器-解码器网络学习图像的全局内容，另一个数据流则用于学习图像边缘细节信息。这种方法类似于传统的反锐化掩模方法，即分离图像边缘细节，增强后再与原图像进行融合。<br><img src="https://github.com/wgxuhui/wgxuhui.github.io/blob/master/images/Deep%20Hybrid%20Network.jpg?raw=true" alt="网络结构"><br>内容流采用了残差网络，并在网络的前两层采用了空洞卷积（dilated convolution）来扩展接收域。在网上查了下空洞卷积的好处，就是可以在“不做pooling损失信息的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息。”当然也有坏处，比如会损失信息的连续性，但是因为这个网络里还有一个边缘流，所以可能这方面就不太重要了吧。另外在上采样阶段，这个残差网络还加入了边缘流的信息，以获得更多的细节。<br>边缘流采用的是空间变异RNN（spatially variant RNN），这种方法是2016年在ECCV首次提出，RNN可以模拟一个图像的滤波操作，用DeepCNN训练一个权值p，将隐藏状态转换成当前状态（h[k-1] -&gt; h[k])。但该滤波器是一个归一化的滤波器，也就是增益为1，因此学习出的边缘仍然很黑。所以他们提出了一种改进空间变异RNN，用DeepCNN学习g,p两个参数。h[k]=g[k]x[k]+p[k]h[k-1]。他们在四个方向上都进行了这种计算。<br><img src="https://github.com/wgxuhui/wgxuhui.github.io/blob/master/images/edge%20stream.png?raw=true" alt="边缘流的网络"><br>损失函数：MSE损失（ MSE loss）、对抗损失（adversarial loss）和感知损失（perceptual loss）。感知损失采用VGG-16，对抗损失采用discriminator。最后再把三个损失函数加起来。<br>数据集：MIT-Adobe FiveK。从5000张图片找到336张低光照图片对，这种低光照指的是在日出或夕阳时的照片。图片加了γ=3的伽马校正和1%的高斯噪声。对比了WVM、LIME、HDRNet、DSLR-Q的方法。看结果，这种方法确实可以在增强图像质量的同时达到比较自然的效果，在PSNR/SSIM方面也有了一定提升。<br>感言时间：总体而言，这篇论文教会我的东西就是让我了解了太多网络，作为一个新手，感觉全世界的方法这文章都用上了，写的肯定很不容易，我看得也十分不易。此外自己加噪声造数据集的方法我觉得十分不可取，让人感觉像做了一个很复杂的模型，但一旦拉出来溜溜搞不好就要露馅。</p><h3 id="2-EnlightenGAN：-Deep-Light-Enhancement-without-Paired-Supervision"><a href="#2-EnlightenGAN：-Deep-Light-Enhancement-without-Paired-Supervision" class="headerlink" title="2. EnlightenGAN： Deep Light Enhancement without Paired Supervision"></a>2. EnlightenGAN： Deep Light Enhancement without Paired Supervision</h3><p><em>Jiang, Y., Gong, X., Liu, D., Cheng, Y., Fang, C., Shen, X., … &amp; Wang, Z. (2019). EnlightenGAN: Deep Light Enhancement without Paired Supervision. arXiv preprint arXiv:1906.06972.</em></p><p>本文采用无监督学习的对抗生成网络进行图像增强的处理。这篇文章首先提出了以往基于深度学习的图像增强方法的弊端，包括：1)同一视觉场景的败坏图像与地面真值图像(如低光图像对与常光图像对同时捕获)是非常困难的，甚至是不切实际的;2)很多文章从普通图像中合成低光照图像，但这种合成的结果通常不够逼真，导致训练后的模型应用于真实的微光图像时会产生各种伪影;3)在低光图像中可能没有唯一的或定义良好的高光地面真值。针对这些问题，作者采用了GAN网络在低光照图像和正常光图像空间建立非成对映射，而不需要依赖精确成对的图像，使得数据集不必使用合成数据或十分有限的数据量。<br><img src="https://github.com/wgxuhui/wgxuhui.github.io/blob/master/images/enlightenGan.JPG?raw=true" alt="网络结构"><br>首先网络使用了一个generator和两个discriminator作为对抗网络，两个discriminator分别检测整个图像和局部图像的true和false。对于生成器，采用的是U-Net网络，这是一种在很多低级的图像任务中都会用来做训练的网络。为了局部增强图像的亮度，用1-I作为生成器输出的mask，I为归一化后的图像像素，这样就相当于有选择性地增强了暗处。</p><p>文章的主要创新在于损失函数的设定，损失函数是用于鉴别器辨别真伪图像。与上一篇文章类似，这里同样用到了感知损失和对抗损失。但是研究人员在实验中发现感知损失随光照强度的变化不明显，也就是说感知损失是用于约束光强之外的特征，称为称其为自特征保留损失。原本的 VGG-feature distance 是网络输出和ground truth之间的距离，现在文中改成了输出和输入的距离，这样就可以在摆脱ground truth的情况下计算感知损失。另外文章还特别强调了在无groundtruth的情况下进行学习，需要注意自正则化的设定。最终的损失函数有四个，分别是全局对抗损失、局部对抗损失、全局自特征保留损失、局部自特征保留损失。</p><p>文章使用的数据集是Berkeley Deep Driving (BBD-100k)，在合成图像上看着这种方法确实很自然，但是在真实的夜景路拍情况下我并没有看出明显的优势。另外文章还把各种方法处理后的图像送入resnet-50分类器，本文方法比其他方法的分类正确率有微小的提升。</p><p>感言时间：这篇文章通过对损失函数的改进摆脱了对groundtruth的依赖，我觉得对于微光图像的增强还是很有意义的。</p><h3 id="3-Seeing-Motion-in-the-Dark"><a href="#3-Seeing-Motion-in-the-Dark" class="headerlink" title="3. Seeing Motion in the Dark"></a>3. Seeing Motion in the Dark</h3><p>该文章的目标是低光照下的视频处理。<br>这篇UIUC和Intel联合发表的论文和他们之前18年的工作（<em>Chen, C., Chen, Q., Xu, J., &amp; Koltun, V. (2018). Learning to see in the dark. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3291-3300).</em>）紧密相连。在18年的文章里，作者为了研究在低级光照下的图像质量增强，提出了一种端到端训练模式的全卷积网络FCN ，用于处理低亮度图像。引入了一个数据集，包含有原始的低曝光率、低亮度图片，同时还有对应的长曝光率图像。方法能够有效地抑制图像噪声，生成色彩均衡、逼真的图像。</p><p>在19年的这篇文章里，他们如法炮制，开始研究视频的图像增强。他们又采集了一个视频数据集，包括静态和动态两部分，静态数据集会采集对应的groundtruth长曝光数据用于训练，而动态视频作为测试集，且没有相应groundtruth，他们认为模型有能力从静态泛化到动态视频上。数据是原始raw视频，首先经过了比较复杂的预处理，然后送入神经网络。网络模型采用的是ResU-Net的结构，提出了重建损失和自一致性损失。其中重建损失是约束output与groundtruth之间的距离，自一致性损失是为了确保时序一致性。结论就是他们的模型确实可以在动态视频上达到比较好的效果。但是模型也并不是完美，比如会在午夜的月光视频中失效（虽然我不理解是为啥），另外就是他们的视频动态范围不高，其实算是降低了难度。<br><img src="https://github.com/wgxuhui/wgxuhui.github.io/blob/master/images/seeing%20motion%20in%20the%20dark.jpg?raw=true" alt="网络结构"><br>感言时间：这个工作让人感觉做深度学习真简单，只要拍点照片、视频就好了，做底层图像处理连人工打标签的工作都省了。不过我不能说这个工作没有意义，恰恰相反其实比那些复杂的网络有意义多了，至少给我们提供了很好用的数据集。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;blockquote&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Low-Light Image Enhancement via a Deep Hybrid Network &lt;/li&gt;
&lt;li&gt;EnlightenGAN：Deep Light Enhancement
      
    
    </summary>
    
    
    
      <category term="文献整理" scheme="http://yoursite.com/tags/%E6%96%87%E7%8C%AE%E6%95%B4%E7%90%86/"/>
    
      <category term="微光增强" scheme="http://yoursite.com/tags/%E5%BE%AE%E5%85%89%E5%A2%9E%E5%BC%BA/"/>
    
  </entry>
  
</feed>
